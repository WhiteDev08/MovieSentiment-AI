# -*- coding: utf-8 -*-
"""Movie sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yYgZmoc-HcLXe4lqSgI3Gb1NKtN5OBuj
"""

import tensorflow as tf
tf.__version__

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt

df=pd.read_csv('/content/IMDB Dataset.csv')

df.shape

df.head()

df.isnull().sum()

df['review'] = df['review'].str.replace('<br /><br />', '')

df.head()

df.sentiment.value_counts()

import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import download

# Download necessary resources
download('stopwords')
download('wordnet')
download('omw-1.4')

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
critical_terms = {"sad", "cry", "depressed", "hopeless",'am'}  # Add domain-specific terms
stop_words = stop_words - critical_terms

# Add domain-specific stopwords (example: 'life')
additional_stopwords = {'life', 'something', 'anything','aand','abt','ability', 'academic', 'able','account','advance'}  # Add more if needed
stop_words.update(additional_stopwords)

# Preprocessing Function
def preprocess_text(text):
    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Remove URLs
    # Remove non-alphabetic characters and punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Remove underscores
    text = re.sub(r'_+', '', text)
    # Remove excessive repeated characters (e.g., "aaaa" -> "aa")
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)

    # Convert to lowercase
    text = text.lower()

    # Tokenize and remove stopwords
    words = text.split()
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    # Remove short words (e.g., single characters)
    words = [word for word in words if len(word) > 2]

    # Lemmatize words
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join cleaned words into a single string
    return ' '.join(words).strip()

# Apply Preprocessing to the Entire Dataset
corpus = [preprocess_text(text) for text in df['review']]

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional,Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Tokenize the text data
tokenizer = Tokenizer(num_words=5000)  # Adjust num_words as needed
tokenizer.fit_on_texts(corpus)
sequences = tokenizer.texts_to_sequences(corpus)

# Pad sequences to ensure uniform length
max_length = max([len(seq) for seq in sequences]) # Get the length of the longest sequence
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')


label_encoder = LabelEncoder()  # Initialize LabelEncoder
labels = label_encoder.fit_transform(df['sentiment']) # Fit and transform the labels

# Split the data
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)


# Define the LSTM model
vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size
embedding_dim = 100  # Embedding dimension (adjust as needed)

model = Sequential()

# Embedding Layer
model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))

# First LSTM Layer with return_sequences=True to stack another LSTM
model.add(Bidirectional(LSTM(128, return_sequences=True)))
model.add(Dropout(0.3))  # Dropout to reduce overfitting

# Second LSTM Layer
model.add(Bidirectional(LSTM(64, return_sequences=True)))
model.add(Dropout(0.3))

# Third LSTM Layer
model.add(Bidirectional(LSTM(32)))

# Fully Connected Layers
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))  # More dropout for regularization
model.add(Dense(32, activation='relu'))

# Output Layer
model.add(Dense(1, activation='sigmoid'))


# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)  # Adjust epochs and batch_size


# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print('Test Accuracy:', accuracy)

model.save('movie_sentiment.keras')

import pickle
with open('movie_tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""INPUT"""

import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
# Download necessary resources if not already downloaded
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
critical_terms = {"sad", "cry", "depressed", "hopeless",'am'}  # Add domain-specific terms
stop_words = stop_words - critical_terms

# Add domain-specific stopwords
additional_stopwords = {'life', 'something', 'anything','aand','abt','ability', 'academic', 'able','account','advance'}  # Add more if needed
stop_words.update(additional_stopwords)

# Preprocessing Function (same as before)
def preprocess_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'_+', '', text)
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    text = text.lower()
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [word for word in words if len(word) > 2]
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words).strip()

# Load the tokenizer
with open('movie_tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Load the model
model = load_model('/content/movie_sentiment.keras') # Replace with actual path

max_length = 28 # This should match the max_length used during training


def predict_sentiment(text):
    new_sequence = tokenizer.texts_to_sequences([preprocess_text(text)])
    padded_new_sequence = pad_sequences(new_sequence, maxlen=max_length, padding='post')
    prediction = model.predict(padded_new_sequence)
    sentiment = "Positive" if prediction[0][0] > 0.5 else "Negative"
    return sentiment, prediction[0][0]

# Example usage
user_input = input("Enter a text: ")
sentiment, probability = predict_sentiment(user_input)
print(f"Input Text: {user_input}")
print(f"Predicted Sentiment: {sentiment}")
print(f"Prediction Probability: {probability:.4f}")